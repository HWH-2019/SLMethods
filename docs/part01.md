## 统计学习的特点
- 统计学习以计算机及网络为平台，是建立在计算机及网络上的；
- 统计学习以数据为研究对象，是数据驱动的学科；
- 统计学习的目的是对数据进行预测与分析；
- 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析；
- 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。

##  统计学习的方法

### 统计学习方法概述
- 是基于数据构建概率统计模型从而对数据进行预测与分析。
- 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优模型，是它对已知的训练数据及未知的测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。


## 统计学习的分类

### 基本分类
分为监督学习 (supervised learning)、无监督学习 (unsupervised learning) 和强化学习 (reinforcement learning) 以及半监督学习、主动学习。

- 监督学习是指从标注数据中学习预测模型的机器学习问题；
- 无监督学习则是从无标注数据中学习预测模型的机器学习问题；
- 强化学习是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题；
- 半监督学习是利用标注数据和未标注数据学习预测模型的机器学习问题；
- 主动学习是机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。

### 按模型分类
- 概率模型VS非概率模型 - 概率模型是生成模型，非概率模型是判别模型
- 线性模型VS非线性模型
- 参数化模型VS非参数化模型 - 参数化模型参数的维度固定、非参数化模型参数的维度不固定或者说无穷大

> 概率模型 -- 决策树、朴素贝叶斯、隐马尔科夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型、逻辑斯谛回归（逻辑回归）


> 非概率模型 -- 感知机、支持向量机、k 近邻、AdaBoost、k 均值、潜在语义分析、神经网络、逻辑斯谛回归（逻辑回归）

> 线性模型 -- 感知机、线性支持向量机、k 近邻、k 均值、潜在语义分析

> 非线性模型 -- 核函数支持向量机、AdaBoost、神经网络、深度学习（复杂的神经网络）

> 参数化模型 -- 感知机、朴素贝叶斯、逻辑斯谛回归（逻辑回归）、k 均值、高斯混合模型

> 非参数化模型 -- 决策树、支持向量机、AdaBoost、 k 近邻、潜在语义分析、潜在狄利克雷分配

### 按算法分类
- 在线学习 (online learning) - 指每次接受一个样本，进行预测，之后学习模型，并不断重复该操作的机器学习
- 批量学习 (batch learning) - 一次接受所有数据，学习模型，之后进行预测

### 按技巧分类
- 贝叶斯学习 (Bayesian learning) -- 在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。
- 核方法 (kernel method) -- 使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习

## 统计学习三要素

- 模型的假设空间 -- 模型
- 模型选择的准则 -- 策略
- 模型学习的算法 -- 算法

### 模型
- 模型就是所有要学习的条件概率分布或决策函数
- 模型的假设空间包含所有可能的条件概率分布或决策函数
- 由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。

### 策略
- 按照什么样的准则学习或者选择最优的模型
- 统计学习的目标在于从假设空间中选取最优模型

#### 损失函数和风险函数
- 0-1损失函数 (0-1 loss function)
- 平方损失函数 (quadratic loss function)
- 绝对损失函数 (absolute loss function)
- 对数损失函数 (logarithmic loss function) 或对数似然损失函数 (log-likehood loss function)

学习的目标就是选择期望风险最小的模型。

经验风险（empirical risk）是指模型  关于训练数据集的平均损失，也称经验损失。

#### 经验风险最小化与结构风险最小化
- 经验风险最小化 (empirical risk minimization, ERM) 的策略认为，经验风险最小的模型是最优的模型。例如极大似然估计。

- 结构风险最小化 (structural risk minimization, SRM) 是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化 (regularization)。结构风险在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或罚项 (penalty term)。例如贝叶斯估计中的最大后验概率估计。

- 结构风险最小化的策略认为结构风险最小的模型是最优的模型。

### 算法
- 算法是指学习模型的具体计算方法。
- 统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

## 模型评估与模型选择
- 统计学习的目的是使学到的模型不仅对己知数据而且对未知数据都能有很好的预测能力。
- 不同的学习方法会给出不同的模型。当损失函数给定时，基于损失函数的 模型的训练误差 (training error) 和模型的测试误差 (test error) 就自然成为学习方法评估的标准。
- 训练误差是模型关于训练数据集的平均损失，测试误差是模型关于测试数据集的平均损失。
- 如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合 (over-fitting)。
- 过拟合是指学习时选择的模型所包含的参数过多，以至出现这一模型对己知数据预测得很好，但对未知数据预测得很差的现象，
- 可以说模型选择旨在避免过拟合并提高模型的预测能力。

## 正则化与交叉验证
- 模型选择的典型方法是正则化（regularization）。
- 正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。
- 正则化的作用是选择经验风险与模型复杂度同时较小的模型。
- 正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大，正则化符合奥卡姆剃刀（Occam’s razor）原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释己知数据并且十分简单才是最好的模型，也就是应该选择的模型。

- 另一种常用的模型选择方法是交叉验证（cross validation），随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。
- 训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。
- 在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。
- 交叉验证的基本想法是重复地使用数据：把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。

### 交叉验证方法
- 简单交叉验证
- S 折交叉验证
- 留一交叉验证

## 泛化能力
- 学习方法的泛化能力 (generalization ability) 是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。
- 泛化误差 (generalization error)： 用学到的模型对未知数据预测的误差。泛化误差反映了学习方法的泛化能力，事实上，泛化误差就是所学习到的模型的期望风险。
- 学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为泛化误差上界 (generalization error bound)。
- 泛化误差上界通常具有以下性质：
  - 它是样本容量的函数，当样本容量增加时，泛化上界趋于0；
  - 它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

## 生成模型与判别模型
概率模型是生成模型，非概率模型是判别模型

### 生成方法的特点
- 生成方法可以还原出联合概率分布 $P(X,Y)$，而判别方法则不能
- 生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型
- 当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用

### 判别方法的特点
- 判别方法直接学习的是条件概率 $P(Y|X)$ 或决策函数 $f(X)$，直接面对预测，往往学习的准确率更高
- 由于直接学习 $P(Y|X)$ 或 $f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题

## 监督学习的应用
监督学习的应用主要在三个方面：分类问题、标注问题和回归问题

### 分类问题
- 分类是监督学习的一个核心问题
- 分类问题包括学习和分类两个过程

#### 评价指标
- TP (True Positive) -- 正确的正例，一个实例是正类并且也被判定成正类
- FN (False Negative) -- 错误的反例，漏报，本为正类但判定为假类
- FP (False Positive) -- 错误的正例，误报，本为假类但判定为正类
- TN (True Negative) -- 正确的反例，一个实例是假类并且也被判定成假类
- 精确率 (precision) -- 正确预测为正的占全部预测为正的比例
- 准确率（accuracy）-- 所有的预测正确（正类负类）的占总的比重
- 召回率 (recall) -- 正确预测为正的占全部实际为正的比例
- F-Score -- 权衡精确率 (precision) 和召回率 (recall)
- PR-曲线 -- 以召回率作为横坐标轴，精确率作为纵坐标轴，AP就是PR曲线与X轴围成的图形面积，AP值为1时模型性能最好。


> 有 k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。

### 标注问题
- 标注 (tagging) 也是一个监督学习问题。
- 可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测 (structure prediction) 问题的简单形式
- 标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。
- 标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。
- 注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。

> 有隐马尔可夫模型、条件随机场等。

## 回归问题
- 回归（regression）是监督学习的另一个重要问题。
- 回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。
- 回归模型正是表示从输入变量到输出变量之间映射的函数.
- 回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合己知数据且很好地预测未知数据。
- 最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法 (least squares) 求解。
