## $k$ 近邻法
- $k$ 近邻法 ($k$-nearest neighbor, $k$-NN) 是一种基本分类与回归方法。
- $k$ 近邻法的输入为实例的特征向量，对应于特征空间的点，输出为实例的类别，可以取多类
- $k$ 近邻法的基本做法是：对于给定训练实例点和输入实例点，首先确定输入实例点的 $k$ 个最近邻训练实例点，然后利用这 $k$ 个训练实例点的类的多数来预测输入实例点的类
- $k$ 近邻法的三个基本要素：$k$ 值的选择、距离度量及分类决策规则
- $k$ 近邻法 1968 年由 Cover 和 Hart 提出

## $k$ 近邻算法
- 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 $k$ 个实例，这 $k$ 个实例的多数属于某个类，就把该输入实例分为这个类。
- $k$ 近邻法的特殊情况是 $k$ = 1 的情形，称为最近邻算法。对于输入的实例点（特征向量）$x$ 最邻近点的类作为 $x$ 的类
- $k$ 近邻法没有显示的学习过程

## $k$ 近邻模型
- $k$ 近邻法使用的模型实际上对应于对特征空间的划分

### 模型
-  近邻法中，当训练集、距离度量（如欧氏距离）、 $k$ 值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。
-  特征空间中，对每个训练实例点 $x_i$，距离该点比其他点更近的所有点组成一个区域，叫作单元 (cell)。
-  每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。
-  最近邻法将实例 $x_i$ 的类 $y_i$ 作为其单元中所有点的类标记 (class label)。这样，每个单元的实例点的类别是确定的。

### 距离度量
- 特征空间中两个实例点的距离是两个实例点相似程度的反映
- $k$ 近邻模型的特征空间一般是 $n$ 维实数向量空间 $\mathbb{R}^n$
- 使用的距离是欧式距离，但也可以是其他距离

#### 常用距离
- $L_p$ 距离定义

$$
L_p(x_i,x_j) = (\sum^{n}_{l=1}|x^{(l)}_i-x^{(l)}_j|^p)^{\frac{1}{p}}
$$

- $p \geq 1$，当 $p=2$ 时，称为欧式距离 (Euclidean distance)，即

$$
L_2(x_i,x_j) = (\sum^{n}_{l=1}|x^{(l)}_i-x^{(l)}_j|^2)^{\frac{1}{2}}
$$

- 当 $p=1$ 时，称为曼哈顿距离 (Manhattan distance)，即

$$
L_1(x_i,x_j) = (\sum^{n}_{l=1}|x^{(l)}_i-x^{(l)}_j|)
$$

- 当 $p=\infty$ 时，称为切比雪夫距离 (Chebyshev distance)，即

$$
L_\infty(x_i,x_j) = (\max_l|x^{(l)}_i-x^{(l)}_j|)
$$

#### $k$ 值的选择
- 如果选择较小的 $k$ 值，就相当于用较小的邻域中的训练实例进行预测， “学习” 的近似误差 (approximation error) 会减小， 只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。
- 如果选择较大的 $k$ 值，就相当于用较大邻域中的训练、实例进行预测，其优点是可以减少学习的估计误差， 但缺点是学习的近似误差会增大。
- 如果 $k=N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。
- 在应用中， $k$ 值一般取一个比较小的数值。
- 通常采用交叉验证法来选取最优的 $k$ 值。

#### 分类决策规则
- $k$ 近邻法中的分类决策规则往往是多数表决，即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。
- 多数表决规则等价于经验风险最小化

### $k$ 近邻法的实现：$kd$ 树
- $kd$ 树是一种对 $k$ 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构
- $kd$ 树是二叉树，表示对 $k$ 维空间的一个划分
- 相当于不断地用垂直与坐标轴的超平面将 $k$ 维空间切分，构成一系列的 $k$ 维超矩形区域

#### 构造 $kd$ 树
- 开始，构造根结点，先以 $x^{(1)}$ 为坐标轴，选取中位数为切点，将根节点对应的矩形区域切分为两部分，切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现。
- 由根结点生成深度为 $1$ 的左、右子结点：左子结点对应坐标 $x^{(1)}$ 小于切分点的子区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域。
- 重复：对深度为 j 的结点，选择 $x^{(l)}$ 为切分的坐标轴，其中 $l = j(mod k) + 1$。
- 由该结点生成深度为 $j+1$ 的左、右子结点：左子结点对应坐标 $x^{(l)}$ 小于切分点的子区域，右子结点对应坐标 $x^{(l)}$ 大于切分点的子区域。
- 直到两个子区域没有实例存在时停止。



#### 搜索 $kd$ 树
- 简单来说，就是先通过构造好的 $kd$ 树，向下查找，找到包含该点的叶节点，
- 然后以该点为圆心，两点之间的距离为半径画圆。真正的邻近点一定在这个圆的内部。
- 我们依次返回这个叶节点的父节点和另一子节点，如果均不在，则返回父节点的父节点即它的另一子节点，直到返回到根节点。
- 如果圆内没有其他的点，则这个叶节点就是该点的最近邻。如果有则更近的那个便是该点的最近邻。

> 具体过程建议参考书本 P56，并学习例题 3.3

> 利用 $kd$ 树可以省去对大部分数据点的搜索，从而减少搜索的计算量，平均复杂度为 $O(logN)$

> $kd$ 树更适用于训练实例数远大于空间维数时的 $k$ 近邻搜索

> 当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描
